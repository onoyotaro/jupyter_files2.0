{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 血糖値　分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import time\n",
    "\n",
    "import pandas\n",
    "import argparse\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "\n",
    "from chainer.datasets import tuple_dataset\n",
    "from chainer.dataset import convert, concat_examples\n",
    "\n",
    "import numpy\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)  # n_units -> n_out\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easy_chainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_list = sorted(glob.glob(\"C:/Users/Owner/Desktop/LAB/LAB 2019/analysis/mix_fabric/CP*F*\"))\n",
    "back_list = sorted(glob.glob(\"C:/Users/Owner/Desktop/LAB/LAB 2019/analysis/mix_fabric/CP*B*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pandas.read_excel(\"C:/Users/Owner/Desktop/blood_glucose.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_list = sorted(glob.glob(\"C:/Users/Owner/Desktop/LAB/LAB 2019/analysis/mix_fabric/CP*.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "xls = pandas.read_csv(front_list[0], skiprows=4, delimiter=\"\\t\", header=None)\n",
    "col = xls.as_matrix()[:,0]\n",
    "\n",
    "j = 0\n",
    "for list_ in (front_list, back_list):\n",
    "#     print(list_)\n",
    "    for i, fname in enumerate (list_):\n",
    "        xls = pandas.read_csv(fname, comment=\"#\", delimiter=\"\\t\", header=None)\n",
    "        if j == 0:\n",
    "            arr = xls.as_matrix()[:,1]\n",
    "            hddr = os.path.split(fname)[1].split(\".\")[0]\n",
    "        else:\n",
    "            arr = numpy.vstack((arr, xls.as_matrix()[:,1]))\n",
    "            hddr = numpy.hstack((hddr, os.path.split(fname)[1].split(\".\")[0]))\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1249, 1713) (1713,) (1249,)\n"
     ]
    }
   ],
   "source": [
    "print(arr.shape, col.shape, hddr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Owner/Desktop/LAB/LAB 2019/analysis/mix_fabric\\\\CP01F1.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "front_list[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 4. 4. 4. 4.\n",
      " 4. 4. 0. 0. 1. 1. 2. 2. 3. 3. 4. 4.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Desktop\\LAB\\LAB 2018\\jupyter File\\easy_chainer.py:87: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  data = xls.as_matrix()[:-1]\n",
      "C:\\Users\\Owner\\Desktop\\LAB\\LAB 2018\\jupyter File\\easy_chainer.py:88: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  teach = xls.as_matrix()[-1]\n"
     ]
    }
   ],
   "source": [
    "data, teach = easy_chainer.load_Data(\"C:/Users/Owner/Desktop/blood_glucose.xls\")\n",
    "data = data.astype(numpy.float32)\n",
    "teach = teach\n",
    "print(teach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9, 11, 16, 16,  8], dtype=int64),\n",
       " array([0. , 0.8, 1.6, 2.4, 3.2, 4. ]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teach = teach.astype(numpy.int8)\n",
    "teach_bins = numpy.histogram(teach, bins=numpy.max(teach) + 1)\n",
    "teach_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "\n",
    "\n",
    "for i in range(int(max(teach) + 1)):\n",
    "    tmp0_arr = numpy.where(teach == i)[0]\n",
    "    try:\n",
    "        tmp0 = numpy.random.choice(tmp0_arr, int(k), replace=False)\n",
    "        if i == 0:\n",
    "            id_train = tmp0\n",
    "        else:\n",
    "            id_train = numpy.hstack((id_train, tmp0))\n",
    "    except Exception:\n",
    "        print(\"Not teach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  2.,  2.,  3.,  0.,  2.,  3.,  1.,  2.,  2.,  1.,  2.,  4.,\n",
       "        0.,  0.,  0.,  4.,  0.,  1.,  1.,  1.,  3.,  1.,  3.,  4.,  3.,\n",
       "        4.,  3.,  3.,  0.,  3.,  3.,  2.,  3.,  3.,  1.,  2.,  2.,  0.,\n",
       "        1.,  2.,  3.,  2.,  1.,  4.,  0.,  4.,  1.,  2.,  2., nan, nan],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teach[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59]\n"
     ]
    }
   ],
   "source": [
    "id_all = numpy.arange(1, len(teach) + 1, 1).astype(numpy.int32) - 1\n",
    "print(id_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44 10 29 16 40  5 25 13 48 50  1  8 51 22 45  3 56 32 17  4 31 26 27 58\n",
      " 33 47 35 28 20  2  7 36 15 18 38 21 54 41 59  9 23 43 24 11  0 39 30 34\n",
      " 19 55]\n"
     ]
    }
   ],
   "source": [
    "id_train = numpy.random.choice(id_all, 50, replace=False) #重複なし\n",
    "print(id_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "teach = teach.astype(numpy.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 12 14 37 42 46 49 52 53 57]\n"
     ]
    }
   ],
   "source": [
    "id_test = numpy.delete(id_all, id_train)\n",
    "print(id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = data[:, id_train], teach[id_train]\n",
    "x_test, y_test = data[:, id_test], teach[id_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80.9, 82.2, 48.2, 63. , 53.1, 32.1, 20. , 22.3, 20.7, 80.7, 45.2,\n",
       "       63.9, 66. , 63.9, 82.8, 82.5, 65.7, 15.3, 85.2, 79.5, 21.9, 83.2,\n",
       "       27.4, 65.7, 68.4, 36. , 65.8, 20.2, 65.6, 62. , 63.7, 64.6, 83.3,\n",
       "       81.9, 83.9, 81.5, 70.4, 19.9, 84.2,  9. , 37.6, 65.6, 66.5, 63.6,\n",
       "       61.2, 19.4, 63. , 91.6, 82.7, 32.3, 66.3, 76.7, 65.6, 49.6, 53.8,\n",
       "       81.6, 77.8,  0.4, 47.2, 21.2, 69.6, 81.2, 63.4, 65.8, 83.1, 81.9,\n",
       "       82.6, 78.5, 82.3, 62.5, 83.5, 42.4, 82.2, 83.4, 77.9, 78.1, 65.4,\n",
       "       66. , 82.4, 66.4, 90.5, 38.3, 62.3, 63.8, 64.3, 62.3, 62.9, 61.3,\n",
       "       27.9, 20.2, 50.3, 62.1, 62.5, 50.3, 91.2, 65.6, 79.6, 64.4, 72.6,\n",
       "       55.3], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tuple_dataset.TupleDataset(x_train.T, y_train.reshape(-1, ))\n",
    "test = tuple_dataset.TupleDataset(x_test.T, y_test.reshape(-1, ))\n",
    "\n",
    "train_iter = chainer.iterators.SerialIterator(train, 50)\n",
    "test_iter = chainer.iterators.SerialIterator(test, 10, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tuple_dataset.TupleDataset(x_train.T, y_train.reshape(-1, ))\n",
    "test = tuple_dataset.TupleDataset(x_test.T, y_test.reshape(-1, ))\n",
    "\n",
    "train_iter = chainer.iterators.SerialIterator(train, 100)\n",
    "test_iter = chainer.iterators.SerialIterator(test, 27, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)  # n_units -> n_out\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = L.Classifier(MLP(1000,1),\n",
    "                     lossfun=F.mean_squared_error,\n",
    "                     accfun=F.r2_score)\n",
    "model.compute_accuracy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = (MLP(1000,1)\n",
    "loss = F.mean_square_error(model(x_train),y_train)\n",
    "accfun = F.r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分類に使う\n",
    "net = MLP(1000, 5)\n",
    "model = L.Classifier(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.sgd.SGD at 0x208c10ffa90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = chainer.optimizers.SGD()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = training.updaters.StandardUpdater(train_iter, optimizer)\n",
    "trainer = training.Trainer(updater, (500, 'epoch'), out=\"Result2018_oono/%s\" % time.strftime(\"%Y%m%d%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with the test dataset for each epoch\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))\n",
    "\n",
    "# Dump a computational graph from 'loss' variable at the first iteration\n",
    "# The \"main\" refers to the target link of the \"main\" optimizer.\n",
    "trainer.extend(extensions.dump_graph('main/loss'))\n",
    "\n",
    "# Take a snapshot for each specified epoch\n",
    "# frequency = args.epoch if args.frequency == -1 else max(1, args.frequency)\n",
    "# trainer.extend(extensions.snapshot(), trigger=(frequency, 'epoch'))\n",
    "\n",
    "# Write a log of evaluation statistics for each epoch\n",
    "trainer.extend(extensions.LogReport())\n",
    "\n",
    "# Save two plot images to the result dir\n",
    "trainer.extend(\n",
    "    extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
    "                          'epoch', file_name='loss.png'))\n",
    "trainer.extend(\n",
    "    extensions.PlotReport(\n",
    "        ['main/accuracy', 'validation/main/accuracy'],\n",
    "        'epoch', file_name='accuracy.png'))\n",
    "\n",
    "# Print selected entries of the log to stdout\n",
    "# Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
    "# \"validation\" refers to the default name of the Evaluator extension.\n",
    "# Entries other than 'epoch' are reported by the Classifier link, called by\n",
    "# either the updater or the evaluator.\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'main/loss', 'validation/main/loss',\n",
    "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "\n",
    "# Print a progress bar to stdout\n",
    "trainer.extend(extensions.ProgressBar())\n",
    "\n",
    "# if args.resume:\n",
    "#     # Resume from a snapshot\n",
    "#     chainer.serializers.load_npz(args.resume, trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "1           1.56203     1.79455               0.32           0                         0.112681      \n",
      "2           1.54656     1.78719               0.32           0                         1.11974       \n",
      "3           1.53862     1.78434               0.32           0                         1.45918       \n",
      "4           1.5339      1.78277               0.32           0                         1.80933       \n",
      "5           1.53044     1.78261               0.32           0                         2.12157       \n",
      "6           1.52752     1.7809                0.32           0                         2.45649       \n",
      "7           1.52483     1.77956               0.32           0                         2.78914       \n",
      "8           1.52245     1.77824               0.32           0                         3.11816       \n",
      "9           1.52051     1.77698               0.32           0                         3.45776       \n",
      "10          1.51874     1.77598               0.32           0                         3.78812       \n",
      "11          1.51709     1.77472               0.32           0                         4.1116        \n",
      "12          1.51558     1.77414               0.32           0                         4.45294       \n",
      "13          1.51414     1.77325               0.32           0                         4.77129       \n",
      "14          1.5127      1.77228               0.32           0                         5.12375       \n",
      "15          1.51136     1.77152               0.32           0                         5.4485        \n",
      "16          1.51009     1.77055               0.32           0                         5.77736       \n",
      "17          1.50889     1.76955               0.32           0                         6.12696       \n",
      "18          1.50775     1.76881               0.32           0                         6.45945       \n",
      "19          1.50665     1.76824               0.32           0                         6.78288       \n",
      "20          1.50556     1.76774               0.32           0                         7.12173       \n",
      "21          1.50449     1.76707               0.32           0                         7.48334       \n",
      "22          1.50344     1.76649               0.32           0                         7.88225       \n",
      "23          1.50242     1.76586               0.32           0                         8.1976        \n",
      "24          1.50139     1.76508               0.32           0                         8.50948       \n",
      "25          1.5004      1.76458               0.32           0                         8.83013       \n",
      "26          1.49944     1.76435               0.32           0                         9.14851       \n",
      "27          1.49848     1.76353               0.32           0                         9.46376       \n",
      "28          1.49756     1.76294               0.32           0                         9.78774       \n",
      "29          1.49663     1.76244               0.32           0                         10.1116       \n",
      "30          1.49571     1.76186               0.32           0                         10.467        \n",
      "31          1.4948      1.76115               0.32           0                         10.8162       \n",
      "32          1.4939      1.76085               0.32           0                         11.2043       \n",
      "33          1.493       1.76044               0.32           0                         11.5304       \n",
      "34          1.49213     1.7598                0.32           0                         11.8692       \n",
      "35          1.49127     1.75929               0.32           0                         12.2055       \n",
      "36          1.49039     1.7585                0.32           0                         12.5374       \n",
      "37          1.48952     1.75847               0.32           0                         12.8749       \n",
      "38          1.48865     1.75759               0.32           0                         13.3175       \n",
      "39          1.48778     1.7571                0.32           0                         13.6822       \n",
      "40          1.4869      1.75654               0.32           0                         14.0696       \n",
      "41          1.48602     1.75586               0.32           0                         14.4297       \n",
      "42          1.48515     1.75547               0.32           0                         14.7944       \n",
      "43          1.48429     1.75488               0.32           0                         15.2105       \n",
      "44          1.48343     1.75476               0.32           0                         15.5172       \n",
      "45          1.48257     1.75396               0.32           0                         15.8361       \n",
      "46          1.48171     1.75352               0.32           0                         16.1581       \n",
      "47          1.48085     1.75276               0.32           0                         16.5031       \n",
      "48          1.48        1.75261               0.32           0                         16.8197       \n",
      "49          1.47916     1.75181               0.32           0                         17.1504       \n",
      "50          1.47832     1.75138               0.32           0                         17.468        \n",
      "51          1.47748     1.75087               0.32           0                         17.787        \n",
      "52          1.47664     1.75017               0.32           0                         18.1175       \n",
      "53          1.47579     1.74955               0.32           0                         18.4418       \n",
      "54          1.47495     1.7489                0.32           0                         18.7684       \n",
      "55          1.4741      1.74873               0.32           0                         19.0837       \n",
      "56          1.47326     1.74781               0.32           0                         19.4319       \n",
      "57          1.4724      1.74744               0.32           0                         19.7584       \n",
      "58          1.47156     1.74692               0.32           0                         20.1743       \n",
      "59          1.47071     1.74634               0.32           0                         20.5216       \n",
      "60          1.46987     1.74578               0.32           0                         20.8566       \n",
      "61          1.46902     1.74545               0.32           0                         21.179        \n",
      "62          1.46818     1.74471               0.32           0                         21.5045       \n",
      "63          1.46734     1.74434               0.32           0                         21.8245       \n",
      "64          1.46649     1.74377               0.32           0                         22.1508       \n",
      "65          1.46565     1.74328               0.32           0                         22.5204       \n",
      "66          1.46482     1.74251               0.32           0                         22.8609       \n",
      "67          1.46398     1.74217               0.32           0                         23.1963       \n",
      "68          1.46315     1.74141               0.32           0                         23.5432       \n",
      "69          1.46232     1.7407                0.32           0                         23.8816       \n",
      "70          1.46148     1.74054               0.32           0                         24.2227       \n",
      "71          1.46063     1.73993               0.32           0                         24.5701       \n",
      "72          1.4598      1.73939               0.32           0                         24.909        \n",
      "73          1.45895     1.73886               0.32           0                         25.2634       \n",
      "74          1.4581      1.73799               0.32           0                         25.6175       \n",
      "75          1.45727     1.73758               0.32           0                         25.9524       \n",
      "76          1.45643     1.73668               0.32           0                         26.4306       \n",
      "77          1.45559     1.73628               0.32           0                         26.7664       \n",
      "78          1.45475     1.73578               0.32           0                         27.1044       \n",
      "79          1.45391     1.73548               0.34           0                         27.4626       \n",
      "80          1.45309     1.73467               0.34           0                         27.8204       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81          1.45224     1.73404               0.34           0                         28.31         \n",
      "82          1.4514      1.73351               0.34           0                         28.7493       \n",
      "83          1.45057     1.73286               0.36           0                         29.1102       \n",
      "84          1.44974     1.73203               0.36           0                         29.4334       \n",
      "85          1.4489      1.73167               0.36           0                         29.7681       \n",
      "86          1.44807     1.73107               0.36           0                         30.1093       \n",
      "87          1.44724     1.73089               0.36           0                         30.4687       \n",
      "88          1.44639     1.73001               0.36           0                         30.8359       \n",
      "89          1.44556     1.72947               0.36           0                         31.1673       \n",
      "90          1.44473     1.72907               0.36           0                         31.5343       \n",
      "91          1.44389     1.72844               0.38           0                         31.8578       \n",
      "92          1.44305     1.72774               0.38           0                         32.1953       \n",
      "93          1.44221     1.72745               0.38           0                         32.5136       \n",
      "94          1.44137     1.72674               0.4            0                         32.8444       \n",
      "95          1.44053     1.72606               0.4            0                         33.1728       \n",
      "96          1.43969     1.72561               0.42           0                         33.5104       \n",
      "97          1.43884     1.72498               0.42           0                         33.835        \n",
      "98          1.438       1.72436               0.42           0                         34.3584       \n",
      "99          1.43716     1.72404               0.42           0                         34.6854       \n",
      "100         1.43632     1.72325               0.42           0                         35.0097       \n",
      "     total [##########........................................] 20.00%\n",
      "this epoch [..................................................]  0.00%\n",
      "       100 iter, 100 epoch / 500 epochs\n",
      "       inf iters/sec. Estimated time to finish: 0:00:00.\n",
      "101         1.43548     1.72304               0.42           0                         35.3361       \n",
      "102         1.43464     1.72256               0.42           0                         35.6802       \n",
      "103         1.43381     1.72185               0.42           0                         35.9977       \n",
      "104         1.43296     1.72131               0.42           0                         36.3258       \n",
      "105         1.43211     1.72078               0.42           0                         36.6564       \n",
      "106         1.43126     1.71996               0.42           0                         36.9797       \n",
      "107         1.43042     1.71957               0.42           0                         37.3125       \n",
      "108         1.42957     1.71904               0.42           0                         37.6768       \n",
      "109         1.42874     1.71852               0.42           0                         38.0132       \n",
      "110         1.42788     1.71797               0.42           0                         38.4108       \n",
      "111         1.42704     1.71716               0.4            0                         38.7832       \n",
      "112         1.4262      1.71683               0.4            0                         39.1118       \n",
      "113         1.42535     1.71617               0.4            0                         39.4433       \n",
      "114         1.42451     1.71554               0.4            0.1                       39.758        \n",
      "115         1.42366     1.7151                0.4            0.1                       40.0791       \n",
      "116         1.42283     1.71444               0.4            0.1                       40.4329       \n",
      "117         1.42199     1.71421               0.4            0.1                       40.7803       \n",
      "118         1.42113     1.71316               0.42           0.1                       41.1168       \n",
      "119         1.42029     1.71321               0.42           0.1                       41.4485       \n",
      "120         1.41945     1.71237               0.42           0.1                       41.7882       \n",
      "121         1.4186      1.71153               0.42           0.1                       42.1253       \n",
      "122         1.41775     1.71102               0.42           0.1                       42.461        \n",
      "123         1.41691     1.71046               0.42           0.1                       42.8214       \n",
      "124         1.41607     1.71002               0.42           0.1                       43.1453       \n",
      "125         1.41522     1.7094                0.42           0.1                       43.5017       \n",
      "126         1.41437     1.70862               0.42           0.1                       44.0559       \n",
      "127         1.41352     1.70833               0.42           0.1                       44.3868       \n",
      "128         1.41268     1.70751               0.42           0.1                       44.7112       \n",
      "129         1.41184     1.7068                0.42           0.1                       45.0404       \n",
      "130         1.411       1.70646               0.42           0.1                       45.3812       \n",
      "131         1.41015     1.70568               0.42           0.1                       45.7164       \n",
      "132         1.4093      1.70492               0.42           0.1                       46.0451       \n",
      "133         1.40846     1.7046                0.42           0.1                       46.3898       \n",
      "134         1.40762     1.70403               0.42           0.1                       46.7453       \n",
      "135         1.40678     1.70317               0.42           0.1                       47.0953       \n",
      "136         1.40593     1.70288               0.42           0.1                       47.4299       \n",
      "137         1.40509     1.70228               0.42           0.1                       47.7938       \n",
      "138         1.40425     1.70192               0.42           0.1                       48.1296       \n",
      "139         1.40342     1.70092               0.42           0.1                       48.4679       \n",
      "140         1.40257     1.7007                0.42           0.1                       48.8056       \n",
      "141         1.40175     1.69958               0.42           0.1                       49.1479       \n",
      "142         1.4009      1.69941               0.42           0.1                       49.5161       \n",
      "143         1.40006     1.69852               0.42           0.1                       49.8527       \n",
      "144         1.39922     1.69841               0.42           0.1                       50.1903       \n",
      "145         1.39839     1.69768               0.42           0.1                       50.5324       \n",
      "146         1.39755     1.69727               0.42           0.1                       50.8754       \n",
      "147         1.3967      1.69628               0.42           0.1                       51.2188       \n",
      "148         1.39586     1.69608               0.42           0.1                       51.5617       \n",
      "149         1.39503     1.69519               0.42           0.1                       51.9033       \n",
      "150         1.39419     1.69483               0.42           0.1                       52.2478       \n",
      "151         1.39336     1.69421               0.42           0.1                       52.6092       \n",
      "152         1.39252     1.69409               0.42           0.1                       52.9539       \n",
      "153         1.39169     1.69324               0.42           0.1                       53.2988       \n",
      "154         1.39085     1.69288               0.42           0.1                       53.6634       \n",
      "155         1.39003     1.69188               0.42           0.1                       54.0116       \n",
      "156         1.38919     1.69144               0.42           0.1                       54.3692       \n",
      "157         1.38836     1.69083               0.42           0.1                       54.7117       \n",
      "158         1.38752     1.69028               0.42           0.1                       55.0587       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159         1.38668     1.68973               0.42           0.1                       55.7203       \n",
      "160         1.38586     1.68935               0.42           0.1                       56.0658       \n",
      "161         1.38504     1.68882               0.42           0.1                       56.4787       \n",
      "162         1.3842      1.68826               0.42           0.1                       56.8356       \n",
      "163         1.38336     1.68781               0.42           0.1                       57.1678       \n",
      "164         1.38251     1.68721               0.42           0.1                       57.5303       \n",
      "165         1.38168     1.68651               0.44           0.1                       57.8593       \n",
      "166         1.38086     1.68621               0.44           0.1                       58.1768       \n",
      "167         1.38004     1.68555               0.44           0.1                       58.5185       \n",
      "168         1.37923     1.68487               0.44           0.1                       58.8518       \n",
      "169         1.3784      1.68418               0.44           0.1                       59.1771       \n",
      "170         1.37758     1.68357               0.44           0.1                       59.507        \n",
      "171         1.37677     1.68299               0.46           0.1                       59.8382       \n",
      "172         1.37597     1.68282               0.46           0.1                       60.1706       \n",
      "173         1.37514     1.6819                0.46           0.1                       60.5004       \n",
      "174         1.37433     1.68164               0.48           0.1                       60.8256       \n",
      "175         1.37353     1.68075               0.48           0.1                       61.1553       \n",
      "176         1.37273     1.68044               0.48           0.1                       61.5096       \n",
      "177         1.37192     1.67948               0.48           0.1                       61.8483       \n",
      "178         1.37112     1.6793                0.48           0.1                       62.1857       \n",
      "179         1.37031     1.67835               0.48           0.1                       62.5156       \n",
      "180         1.3695      1.6779                0.48           0.1                       62.8447       \n",
      "181         1.36871     1.67773               0.48           0.1                       63.1778       \n",
      "182         1.36792     1.67672               0.48           0.1                       63.5302       \n",
      "183         1.36712     1.67642               0.48           0.1                       63.8676       \n",
      "184         1.36633     1.67567               0.48           0.1                       64.1984       \n",
      "185         1.36553     1.67492               0.48           0.1                       64.5475       \n",
      "186         1.36475     1.67493               0.48           0.1                       64.8877       \n",
      "187         1.36396     1.67419               0.48           0.1                       65.2181       \n",
      "188         1.36316     1.6738                0.48           0.1                       65.5514       \n",
      "189         1.36239     1.67319               0.48           0.1                       65.8889       \n",
      "190         1.3616      1.67239               0.48           0.1                       66.2342       \n",
      "191         1.36083     1.6718                0.48           0.1                       66.5887       \n",
      "192         1.36005     1.67145               0.48           0.1                       66.9432       \n",
      "193         1.35927     1.67058               0.48           0.1                       67.2876       \n",
      "194         1.35849     1.67029               0.48           0.1                       67.6497       \n",
      "195         1.35771     1.66977               0.48           0.1                       68.0159       \n",
      "196         1.35694     1.66893               0.48           0.1                       68.3658       \n",
      "197         1.35618     1.66876               0.48           0.1                       68.7009       \n",
      "198         1.3554      1.66773               0.48           0.1                       69.0353       \n",
      "199         1.35463     1.66738               0.48           0.1                       69.3659       \n",
      "200         1.35388     1.66684               0.48           0.2                       69.7056       \n",
      "     total [####################..............................] 40.00%\n",
      "this epoch [..................................................]  0.00%\n",
      "       200 iter, 200 epoch / 500 epochs\n",
      "    2.8522 iters/sec. Estimated time to finish: 0:01:45.181391.\n",
      "201         1.35311     1.66619               0.48           0.2                       70.3987       \n",
      "202         1.35237     1.66582               0.48           0.2                       70.7568       \n",
      "203         1.35161     1.66531               0.48           0.2                       71.0993       \n",
      "204         1.35085     1.66469               0.48           0.2                       71.4345       \n",
      "205         1.35011     1.66395               0.48           0.2                       71.7854       \n",
      "206         1.34936     1.66342               0.48           0.2                       72.1237       \n",
      "207         1.34862     1.66321               0.48           0.2                       72.435        \n",
      "208         1.34788     1.66226               0.48           0.2                       72.7576       \n",
      "209         1.34714     1.66164               0.48           0.2                       73.075        \n",
      "210         1.3464      1.66149               0.48           0.2                       73.3871       \n",
      "211         1.34567     1.6607                0.48           0.2                       73.7213       \n",
      "212         1.34493     1.66038               0.48           0.2                       74.0357       \n",
      "213         1.3442      1.65959               0.48           0.2                       74.3492       \n",
      "214         1.34347     1.65904               0.46           0.2                       74.6826       \n",
      "215         1.34275     1.65875               0.46           0.2                       74.9905       \n",
      "216         1.34203     1.65798               0.46           0.2                       75.317        \n",
      "217         1.3413      1.65795               0.46           0.2                       75.6255       \n",
      "218         1.34059     1.65711               0.46           0.2                       75.9395       \n",
      "219         1.33986     1.65649               0.46           0.2                       76.2489       \n",
      "220         1.33915     1.65599               0.46           0.2                       76.5785       \n",
      "221         1.33844     1.65552               0.46           0.2                       76.9053       \n",
      "222         1.33772     1.65453               0.46           0.2                       77.2133       \n",
      "223         1.33702     1.65451               0.46           0.2                       77.5467       \n",
      "224         1.33632     1.65372               0.46           0.2                       77.8618       \n",
      "225         1.33561     1.6534                0.46           0.2                       78.1738       \n",
      "226         1.33491     1.65277               0.46           0.2                       78.4803       \n",
      "227         1.33422     1.65236               0.46           0.2                       78.7875       \n",
      "228         1.33352     1.65168               0.46           0.2                       79.1042       \n",
      "229         1.33284     1.65124               0.46           0.2                       79.4203       \n",
      "230         1.33216     1.65081               0.46           0.2                       79.7625       \n",
      "231         1.33147     1.65021               0.46           0.2                       80.0776       \n",
      "232         1.33078     1.64998               0.46           0.2                       80.4121       \n",
      "233         1.33011     1.64926               0.46           0.2                       80.7286       \n",
      "234         1.32943     1.64884               0.46           0.2                       81.0437       \n",
      "235         1.32875     1.64818               0.46           0.2                       81.3625       \n",
      "236         1.32808     1.64773               0.46           0.2                       81.7013       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237         1.32741     1.64716               0.46           0.2                       82.0154       \n",
      "238         1.32674     1.64682               0.46           0.2                       82.3373       \n",
      "239         1.32608     1.64594               0.46           0.2                       82.6795       \n",
      "240         1.32542     1.64561               0.46           0.2                       83.0248       \n",
      "241         1.32476     1.64519               0.46           0.2                       83.3454       \n",
      "242         1.32411     1.64457               0.46           0.2                       83.6697       \n",
      "243         1.32344     1.64397               0.46           0.2                       84.0024       \n",
      "244         1.32278     1.64336               0.46           0.2                       84.3261       \n",
      "245         1.32214     1.64324               0.46           0.2                       84.66         \n",
      "246         1.32151     1.64249               0.46           0.2                       84.9883       \n",
      "247         1.32087     1.64214               0.46           0.2                       85.3155       \n",
      "248         1.32024     1.64157               0.46           0.2                       85.6643       \n",
      "249         1.31961     1.64077               0.46           0.2                       85.9943       \n",
      "250         1.31897     1.64044               0.48           0.2                       86.3466       \n",
      "251         1.31835     1.63973               0.48           0.2                       86.6753       \n",
      "252         1.31773     1.6398                0.48           0.2                       87.0054       \n",
      "253         1.3171      1.63875               0.48           0.2                       87.3323       \n",
      "254         1.31648     1.63868               0.48           0.2                       87.6585       \n",
      "255         1.31586     1.63762               0.48           0.2                       87.9991       \n",
      "256         1.31524     1.63759               0.48           0.2                       88.8257       \n",
      "257         1.31463     1.63691               0.48           0.2                       89.1432       \n",
      "258         1.31401     1.63613               0.48           0.2                       89.4732       \n",
      "259         1.31341     1.63619               0.48           0.2                       89.8084       \n",
      "260         1.31281     1.63552               0.48           0.2                       90.137        \n",
      "261         1.31221     1.635                 0.48           0.2                       90.4766       \n",
      "262         1.31161     1.63454               0.48           0.2                       90.8408       \n",
      "263         1.31102     1.63399               0.48           0.2                       91.1642       \n",
      "264         1.31044     1.63355               0.48           0.2                       91.4948       \n",
      "265         1.30984     1.63315               0.48           0.2                       91.8514       \n",
      "266         1.30925     1.63271               0.48           0.2                       92.1881       \n",
      "267         1.30866     1.63201               0.48           0.2                       92.5201       \n",
      "268         1.30808     1.6316                0.48           0.2                       92.8421       \n",
      "269         1.30749     1.631                 0.48           0.2                       93.1747       \n",
      "270         1.30691     1.6307                0.48           0.2                       93.4957       \n",
      "271         1.30634     1.63028               0.48           0.2                       93.8287       \n",
      "272         1.30577     1.62999               0.48           0.2                       94.1514       \n",
      "273         1.3052      1.62921               0.48           0.2                       94.4871       \n",
      "274         1.30463     1.62902               0.48           0.2                       94.8641       \n",
      "275         1.30408     1.62802               0.48           0.2                       95.1915       \n",
      "276         1.30353     1.62798               0.48           0.2                       95.5654       \n",
      "277         1.30298     1.6271                0.48           0.2                       95.9273       \n",
      "278         1.30243     1.62707               0.48           0.2                       96.281        \n",
      "279         1.30188     1.62673               0.48           0.2                       96.643        \n",
      "280         1.30135     1.62611               0.48           0.2                       96.9904       \n",
      "281         1.3008      1.62555               0.48           0.2                       97.3202       \n",
      "282         1.30026     1.62519               0.48           0.2                       97.6845       \n",
      "283         1.29973     1.62466               0.48           0.2                       98.0385       \n",
      "284         1.29919     1.6242                0.48           0.2                       98.3621       \n",
      "285         1.29867     1.62415               0.48           0.2                       98.6951       \n",
      "286         1.29815     1.62334               0.48           0.2                       99.0334       \n",
      "287         1.29761     1.62288               0.48           0.2                       99.3594       \n",
      "288         1.29708     1.6223                0.48           0.2                       99.7011       \n",
      "289         1.29656     1.62199               0.48           0.2                       100.046       \n",
      "290         1.29605     1.62151               0.48           0.2                       100.378       \n",
      "291         1.29554     1.62115               0.48           0.2                       100.749       \n",
      "292         1.29503     1.62073               0.48           0.2                       101.109       \n",
      "293         1.29453     1.62006               0.48           0.2                       101.464       \n",
      "294         1.29403     1.61985               0.48           0.2                       101.806       \n",
      "295         1.29352     1.61918               0.48           0.2                       102.137       \n",
      "296         1.29302     1.61905               0.48           0.2                       102.477       \n",
      "297         1.29253     1.61831               0.48           0.2                       102.811       \n",
      "298         1.29203     1.61829               0.48           0.2                       103.154       \n",
      "299         1.29155     1.61796               0.48           0.2                       103.499       \n",
      "300         1.29105     1.61725               0.48           0.2                       103.872       \n",
      "     total [##############################....................] 60.00%\n",
      "this epoch [..................................................]  0.00%\n",
      "       300 iter, 300 epoch / 500 epochs\n",
      "    2.9031 iters/sec. Estimated time to finish: 0:01:08.890794.\n",
      "301         1.29056     1.6168                0.48           0.2                       104.224       \n",
      "302         1.29008     1.61676               0.48           0.2                       104.563       \n",
      "303         1.2896      1.61586               0.48           0.2                       104.907       \n",
      "304         1.28911     1.61562               0.48           0.2                       105.242       \n",
      "305         1.28865     1.61526               0.48           0.2                       105.578       \n",
      "306         1.28818     1.61482               0.48           0.2                       105.916       \n",
      "307         1.2877      1.61401               0.48           0.2                       106.25        \n",
      "308         1.28725     1.61405               0.48           0.2                       106.606       \n",
      "309         1.28678     1.61323               0.48           0.2                       106.979       \n",
      "310         1.28631     1.6132                0.48           0.2                       107.319       \n",
      "311         1.28586     1.61289               0.48           0.2                       107.66        \n",
      "312         1.28541     1.6124                0.48           0.2                       108           \n",
      "313         1.28495     1.61189               0.48           0.2                       108.355       \n",
      "314         1.2845      1.61152               0.48           0.2                       108.689       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315         1.28405     1.61113               0.48           0.2                       109.034       \n",
      "316         1.28359     1.61064               0.48           0.2                       109.377       \n",
      "317         1.28314     1.61012               0.48           0.2                       109.743       \n",
      "318         1.2827      1.61001               0.48           0.2                       110.093       \n",
      "319         1.28226     1.60968               0.48           0.2                       110.428       \n",
      "320         1.28181     1.60913               0.48           0.2                       110.769       \n",
      "321         1.28137     1.60886               0.48           0.2                       111.114       \n",
      "322         1.28093     1.60818               0.48           0.2                       112.021       \n",
      "323         1.28049     1.60792               0.48           0.2                       112.354       \n",
      "324         1.28006     1.60754               0.48           0.2                       112.728       \n",
      "325         1.27963     1.6075                0.48           0.2                       113.07        \n",
      "326         1.27921     1.60658               0.48           0.2                       113.421       \n",
      "327         1.27877     1.60632               0.48           0.2                       113.776       \n",
      "328         1.27836     1.60619               0.48           0.2                       114.126       \n",
      "329         1.27793     1.60536               0.48           0.2                       114.467       \n",
      "330         1.27751     1.60546               0.48           0.2                       114.817       \n",
      "331         1.27709     1.60453               0.48           0.2                       115.229       \n",
      "332         1.27668     1.60466               0.48           0.2                       115.702       \n",
      "333         1.27625     1.60423               0.48           0.2                       116.234       \n",
      "334         1.27584     1.60393               0.48           0.2                       116.694       \n",
      "335         1.27541     1.60342               0.48           0.2                       117.171       \n",
      "336         1.275       1.60301               0.48           0.2                       117.593       \n",
      "337         1.27458     1.60268               0.48           0.2                       118.071       \n",
      "338         1.27417     1.60222               0.48           0.2                       118.527       \n",
      "339         1.27376     1.60179               0.48           0.2                       118.977       \n",
      "340         1.27336     1.60146               0.48           0.2                       119.344       \n",
      "341         1.27295     1.60113               0.48           0.2                       119.711       \n",
      "342         1.27257     1.60076               0.48           0.2                       120.087       \n",
      "343         1.27216     1.6002                0.48           0.2                       120.476       \n",
      "344         1.27178     1.6001                0.48           0.2                       120.843       \n",
      "345         1.27139     1.59917               0.48           0.2                       121.208       \n",
      "346         1.27102     1.5992                0.5            0.2                       121.568       \n",
      "347         1.27064     1.59849               0.48           0.2                       121.947       \n",
      "348         1.27025     1.59845               0.5            0.2                       122.296       \n",
      "349         1.26987     1.59788               0.5            0.2                       122.641       \n",
      "350         1.26948     1.59746               0.5            0.2                       123.019       \n",
      "351         1.26909     1.59719               0.5            0.2                       123.391       \n",
      "352         1.2687      1.59668               0.5            0.2                       123.777       \n",
      "353         1.26834     1.59642               0.5            0.2                       124.149       \n",
      "354         1.26796     1.5958                0.5            0.2                       124.516       \n",
      "355         1.26759     1.59549               0.5            0.2                       124.91        \n",
      "356         1.26722     1.5952                0.5            0.2                       125.266       \n",
      "357         1.26686     1.595                 0.5            0.2                       125.642       \n",
      "358         1.26649     1.59438               0.5            0.2                       125.986       \n",
      "359         1.26613     1.59378               0.5            0.2                       126.343       \n",
      "360         1.26578     1.59335               0.5            0.2                       126.749       \n",
      "361         1.26542     1.59333               0.5            0.2                       127.139       \n",
      "362         1.26506     1.59278               0.5            0.2                       127.553       \n",
      "363         1.26471     1.59252               0.5            0.2                       127.961       \n",
      "364         1.26437     1.5919                0.5            0.2                       128.538       \n",
      "365         1.26401     1.59168               0.5            0.2                       128.947       \n",
      "366         1.26367     1.5911                0.5            0.2                       129.295       \n",
      "367         1.26333     1.59095               0.5            0.2                       129.647       \n",
      "368         1.26298     1.59043               0.5            0.2                       130.009       \n",
      "369         1.26265     1.59065               0.5            0.2                       130.361       \n",
      "370         1.26232     1.58992               0.5            0.2                       130.721       \n",
      "371         1.26199     1.58958               0.5            0.2                       131.111       \n",
      "372         1.26166     1.58905               0.5            0.2                       131.469       \n",
      "373         1.26133     1.58909               0.5            0.2                       131.885       \n",
      "374         1.26101     1.58823               0.5            0.2                       132.251       \n",
      "375         1.26069     1.58826               0.5            0.2                       132.611       \n",
      "376         1.26037     1.58793               0.5            0.2                       132.96        \n",
      "377         1.26005     1.58781               0.5            0.2                       133.312       \n",
      "378         1.25973     1.58713               0.5            0.2                       133.659       \n",
      "379         1.25941     1.58711               0.5            0.2                       134.058       \n",
      "380         1.25909     1.58636               0.5            0.2                       134.411       \n",
      "381         1.25877     1.58634               0.5            0.2                       134.767       \n",
      "382         1.25846     1.58596               0.5            0.2                       135.13        \n",
      "383         1.25814     1.58559               0.5            0.2                       135.488       \n",
      "384         1.25784     1.58549               0.5            0.2                       135.84        \n",
      "385         1.25753     1.58511               0.5            0.2                       136.205       \n",
      "386         1.25722     1.58432               0.5            0.2                       136.565       \n",
      "387         1.2569      1.58435               0.5            0.2                       136.946       \n",
      "388         1.2566      1.58389               0.5            0.2                       137.314       \n",
      "389         1.2563      1.58377               0.5            0.2                       137.666       \n",
      "390         1.256       1.58342               0.5            0.2                       138.015       \n",
      "391         1.2557      1.58308               0.5            0.2                       138.471       \n",
      "392         1.2554      1.58256               0.5            0.2                       138.891       \n",
      "393         1.25511     1.58219               0.5            0.2                       139.268       \n",
      "394         1.25481     1.5819                0.5            0.2                       139.626       \n",
      "395         1.25451     1.5819                0.5            0.2                       140.041       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396         1.25422     1.58126               0.5            0.2                       140.401       \n",
      "397         1.25393     1.58133               0.5            0.2                       140.786       \n",
      "398         1.25364     1.58059               0.5            0.2                       141.152       \n",
      "399         1.25335     1.58064               0.5            0.2                       142.265       \n",
      "400         1.25307     1.57974               0.5            0.2                       142.685       \n",
      "     total [########################################..........] 80.00%\n",
      "this epoch [..................................................]  0.00%\n",
      "       400 iter, 400 epoch / 500 epochs\n",
      "    2.7841 iters/sec. Estimated time to finish: 0:00:35.918257.\n",
      "401         1.25278     1.58021               0.5            0.2                       143.086       \n",
      "402         1.25251     1.57923               0.5            0.2                       143.474       \n",
      "403         1.25223     1.5793                0.5            0.2                       143.856       \n",
      "404         1.25193     1.57858               0.5            0.2                       144.283       \n",
      "405         1.25166     1.57864               0.5            0.2                       144.65        \n",
      "406         1.25138     1.57785               0.5            0.2                       145.032       \n",
      "407         1.25109     1.57809               0.5            0.2                       145.415       \n",
      "408         1.25082     1.57746               0.5            0.2                       145.781       \n",
      "409         1.25054     1.57758               0.5            0.2                       146.165       \n",
      "410         1.25026     1.57658               0.5            0.2                       146.542       \n",
      "411         1.24998     1.57685               0.5            0.2                       146.914       \n",
      "412         1.24971     1.57604               0.5            0.2                       147.269       \n",
      "413         1.24944     1.57607               0.5            0.2                       147.619       \n",
      "414         1.24916     1.57527               0.5            0.2                       147.96        \n",
      "415         1.24889     1.57581               0.5            0.2                       148.291       \n",
      "416         1.24862     1.57476               0.5            0.2                       148.644       \n",
      "417         1.24835     1.57494               0.5            0.2                       148.985       \n",
      "418         1.24807     1.57411               0.5            0.2                       149.319       \n",
      "419         1.24781     1.57417               0.5            0.2                       149.661       \n",
      "420         1.24754     1.57334               0.5            0.2                       150.014       \n",
      "421         1.24728     1.57354               0.5            0.2                       150.383       \n",
      "422         1.24702     1.57291               0.5            0.2                       150.77        \n",
      "423         1.24676     1.57315               0.5            0.2                       151.11        \n",
      "424         1.2465      1.5723                0.5            0.2                       151.452       \n",
      "425         1.24624     1.5722                0.5            0.2                       151.781       \n",
      "426         1.24599     1.57167               0.5            0.2                       152.122       \n",
      "427         1.24574     1.57157               0.5            0.2                       152.454       \n",
      "428         1.24549     1.57121               0.5            0.2                       152.785       \n",
      "429         1.24523     1.57103               0.5            0.2                       153.151       \n",
      "430         1.24498     1.57083               0.5            0.2                       153.504       \n",
      "431         1.24473     1.5702                0.5            0.2                       153.835       \n",
      "432         1.24449     1.57016               0.5            0.2                       154.169       \n",
      "433         1.24424     1.56992               0.5            0.2                       154.499       \n",
      "434         1.24399     1.56965               0.5            0.2                       154.831       \n",
      "435         1.24375     1.56909               0.5            0.2                       155.193       \n",
      "436         1.2435      1.56898               0.5            0.2                       155.593       \n",
      "437         1.24326     1.56854               0.5            0.2                       155.935       \n",
      "438         1.24302     1.5687                0.5            0.2                       156.313       \n",
      "439         1.24278     1.56796               0.5            0.2                       156.655       \n",
      "440         1.24253     1.56786               0.5            0.2                       157.014       \n",
      "441         1.24229     1.56735               0.5            0.2                       157.406       \n",
      "442         1.24205     1.5671                0.5            0.2                       157.789       \n",
      "443         1.24182     1.56655               0.5            0.2                       158.123       \n",
      "444         1.24158     1.56686               0.5            0.2                       158.498       \n",
      "445         1.24135     1.56625               0.5            0.2                       158.882       \n",
      "446         1.24111     1.56611               0.5            0.2                       159.297       \n",
      "447         1.24087     1.56529               0.5            0.2                       159.666       \n",
      "448         1.24064     1.56565               0.5            0.2                       160.025       \n",
      "449         1.2404      1.56478               0.5            0.2                       160.738       \n",
      "450         1.24016     1.56476               0.5            0.2                       161.073       \n",
      "451         1.23993     1.56476               0.5            0.2                       161.416       \n",
      "452         1.2397      1.56433               0.5            0.2                       161.752       \n",
      "453         1.23946     1.56405               0.5            0.2                       162.111       \n",
      "454         1.23922     1.56392               0.5            0.2                       162.465       \n",
      "455         1.23899     1.56319               0.5            0.2                       162.803       \n",
      "456         1.23876     1.56318               0.5            0.2                       163.139       \n",
      "457         1.23853     1.5623                0.5            0.2                       163.492       \n",
      "458         1.23831     1.56265               0.5            0.2                       163.843       \n",
      "459         1.23808     1.56204               0.5            0.2                       164.183       \n",
      "460         1.23786     1.56234               0.5            0.2                       164.515       \n",
      "461         1.23764     1.56133               0.5            0.2                       164.867       \n",
      "462         1.23741     1.56161               0.5            0.2                       165.229       \n",
      "463         1.23719     1.56066               0.5            0.2                       165.583       \n",
      "464         1.23696     1.56089               0.5            0.2                       165.966       \n",
      "465         1.23675     1.5603                0.5            0.2                       166.388       \n",
      "466         1.23653     1.56028               0.5            0.2                       166.739       \n",
      "467         1.23631     1.55988               0.5            0.2                       167.088       \n",
      "468         1.23609     1.55984               0.5            0.2                       167.443       \n",
      "469         1.23587     1.55924               0.5            0.2                       167.789       \n",
      "470         1.23565     1.55883               0.5            0.2                       168.156       \n",
      "471         1.23544     1.55882               0.5            0.2                       168.509       \n",
      "472         1.23522     1.55872               0.5            0.2                       168.856       \n",
      "473         1.235       1.55847               0.5            0.2                       169.205       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474         1.23478     1.55781               0.5            0.2                       169.539       \n",
      "475         1.23457     1.55794               0.5            0.2                       169.878       \n",
      "476         1.23436     1.55737               0.5            0.2                       170.229       \n",
      "477         1.23415     1.55717               0.5            0.2                       170.569       \n",
      "478         1.23393     1.55717               0.5            0.2                       170.905       \n",
      "479         1.23373     1.55702               0.5            0.2                       171.289       \n",
      "480         1.23351     1.55651               0.5            0.2                       171.657       \n",
      "481         1.23329     1.55611               0.5            0.2                       172.018       \n",
      "482         1.23308     1.55616               0.5            0.2                       172.388       \n",
      "483         1.23287     1.55571               0.5            0.2                       172.823       \n",
      "484         1.23266     1.55523               0.5            0.2                       173.182       \n",
      "485         1.23245     1.55491               0.5            0.2                       173.586       \n",
      "486         1.23225     1.55495               0.5            0.2                       173.983       \n",
      "487         1.23203     1.55451               0.5            0.2                       174.42        \n",
      "488         1.23182     1.55455               0.5            0.2                       174.808       \n",
      "489         1.23163     1.55403               0.5            0.2                       175.186       \n",
      "490         1.23143     1.55377               0.5            0.2                       175.527       \n",
      "491         1.23122     1.55362               0.5            0.2                       175.906       \n",
      "492         1.23101     1.55312               0.5            0.2                       176.268       \n",
      "493         1.23081     1.55299               0.5            0.2                       176.663       \n",
      "494         1.23061     1.55274               0.5            0.2                       177.068       \n",
      "495         1.23041     1.55288               0.5            0.2                       177.483       \n",
      "496         1.2302      1.55194               0.5            0.2                       177.853       \n",
      "497         1.23002     1.55233               0.5            0.2                       178.193       \n",
      "498         1.22981     1.55122               0.5            0.2                       178.533       \n",
      "499         1.22961     1.55171               0.5            0.2                       178.879       \n",
      "500         1.22941     1.55081               0.5            0.2                       179.227       \n",
      "     total [##################################################] 100.00%\n",
      "this epoch [..................................................]  0.00%\n",
      "       500 iter, 500 epoch / 500 epochs\n",
      "    2.7552 iters/sec. Estimated time to finish: 0:00:00.\n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195609, 3)\n"
     ]
    }
   ],
   "source": [
    "csv = pandas.read_csv(\"C:/Users/Owner/Desktop/[190611]kurasawa/Blood_glucose_level/1.csv\", header=None)\n",
    "\n",
    "f = open(\"C:/Users/Owner/Desktop/[190611]kurasawa/Blood_glucose_level/1.csv\",\"r\")\n",
    "csv_list = []\n",
    "csv_list.append(csv)\n",
    "\n",
    "print(csv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 血糖値　回帰予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import time\n",
    "\n",
    "import pandas\n",
    "import argparse\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "\n",
    "from chainer.datasets import tuple_dataset\n",
    "from chainer.dataset import convert, concat_examples\n",
    "\n",
    "import numpy\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from chainer import Sequential\n",
    "import easy_chainer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)  # n_units -> n_out\n",
    "            \n",
    "           \n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5層以上NN\n",
    "# Batch Normalization\n",
    "class MLP2(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP2, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l4 = L.Linear(None, n_units)    # n_units -> n_out\n",
    "            self.l5 = L.Linear(None, n_out)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        dh = F.dropout(h1, 0.8)                       # batch normalization層を作る（試験的に導入,検討を重ねる必要がある）\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        h = F.dropout(h2, 0.5)                       # batch normalization\n",
    "        h3 = F.relu(self.l3(h2))\n",
    "        kh = F.dropout(h3, 0.2)                      # batch normalization\n",
    "        h4 = F.relu(self.l4(h3))\n",
    "        return self.l5(h4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427,)\n",
      "(1000, 427)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Desktop\\LAB\\LAB 2018\\jupyter File\\easy_chainer.py:87: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  data = xls.as_matrix()[:-1]\n",
      "C:\\Users\\Owner\\Desktop\\LAB\\LAB 2018\\jupyter File\\easy_chainer.py:88: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  teach = xls.as_matrix()[-1]\n"
     ]
    }
   ],
   "source": [
    "data, teach = easy_chainer.load_Data(\"C:/Users/Owner/Desktop/Normalized/blood_glucose_fulldata.xlsx\")\n",
    "data = data.astype(numpy.float32)\n",
    "\n",
    "print(teach.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 427)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "teach = teach.astype(numpy.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_all = numpy.arange(1, len(teach) + 1, 1).astype(numpy.int32) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(32)\n",
    "id_train = numpy.random.choice(id_all, 300, replace=False) #重複なし\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test = numpy.delete(id_all, id_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 97.,  89., 150., 134., 145.,  98.,  94.,  98., 102.,  90., 137.,\n",
       "       139., 141., 151., 133., 144., 116., 109., 111., 111., 116., 117.,\n",
       "       128., 121., 156., 179., 154., 145., 149., 149., 157., 160., 135.,\n",
       "       133., 134., 114.,  98., 109., 118., 143., 135., 149., 153., 132.,\n",
       "       132., 120.,  90.,  96.,  95.,  96.,  96., 158., 145., 156., 141.,\n",
       "       120., 128., 132.,  95.,  98., 113., 113., 131., 137., 142., 104.,\n",
       "       161., 164., 167., 186., 188., 189., 156., 167., 137., 140., 151.,\n",
       "        94., 103.,  82., 169., 173., 168., 191., 147., 152., 155., 158.,\n",
       "       128., 131., 100.,  99., 177., 154., 143., 125., 117., 129., 145.,\n",
       "       149., 145., 100.,  96.,  99., 108.,  95.,  97.,  95., 185., 182.,\n",
       "       180., 162., 169., 154., 131., 127., 125., 133., 145., 117., 114.,\n",
       "       185., 210., 190., 204., 220., 180.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teach[id_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,) (28,)\n"
     ]
    }
   ],
   "source": [
    "print(id_train.shape, id_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_train :  (394,)\n",
      "id_train_type :  int32\n",
      "id_test :  (33,)\n",
      "id_test :  int32\n"
     ]
    }
   ],
   "source": [
    "id_train_1 = teach[0:394]\n",
    "id_test_1 = teach[394:427]\n",
    "\n",
    "id_train = id_train_1.astype(numpy.int32)\n",
    "id_test = id_test_1.astype(numpy.int32)\n",
    "\n",
    "# teach = teach.astype(numpy.float32)\n",
    "\n",
    "# print(\"id_train : \", id_train)\n",
    "print(\"id_train : \", id_train.shape)\n",
    "print(\"id_train_type : \", id_train.dtype)\n",
    "\n",
    "# print(\"id_test : \", id_test)\n",
    "print(\"id_test : \", id_test.shape)\n",
    "print(\"id_test : \", id_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = data[:, id_train], teach[id_train]\n",
    "x_test, y_test = data[:, id_test], teach[id_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tuple_dataset.TupleDataset(x_train.T, y_train.reshape(-1,1 ))\n",
    "test = tuple_dataset.TupleDataset(x_test.T, y_test.reshape(-1, 1))\n",
    "\n",
    "train_iter = chainer.iterators.SerialIterator(train, 10, repeat=True, shuffle=False)\n",
    "test_iter = chainer.iterators.SerialIterator(test, 10, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP2(1000,1)\n",
    "model = L.Classifier(net,\n",
    "                     lossfun=F.mean_squared_error,\n",
    "                     accfun = F.r2_score)\n",
    "model.compute_accuracy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0x24f0c0fb978>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimizer = chainer.optimizers.SGD()\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = training.updaters.StandardUpdater(train_iter, optimizer)\n",
    "trainer = training.Trainer(updater, (500, 'epoch'), out=\"Result2018_oono/%s\" % time.strftime(\"%Y%m%d%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = training.updaters.StandardUpdater(train_iter, optimizer)\n",
    "trainer = training.Trainer(updater, (20, 'epoch'), out=\"Result2018_oono/%s\" % time.strftime(\"%Y%m%d%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with the test dataset for each epoch\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))\n",
    "\n",
    "# Dump a computational graph from 'loss' variable at the first iteration\n",
    "# The \"main\" refers to the target link of the \"main\" optimizer.\n",
    "trainer.extend(extensions.dump_graph('main/loss'))\n",
    "\n",
    "# Take a snapshot for each specified epoch\n",
    "# frequency = args.epoch if args.frequency == -1 else max(1, args.frequency)\n",
    "# trainer.extend(extensions.snapshot(), trigger=(frequency, 'epoch'))\n",
    "\n",
    "# Write a log of evaluation statistics for each epoch\n",
    "trainer.extend(extensions.LogReport())\n",
    "\n",
    "# Save two plot images to the result dir\n",
    "trainer.extend(\n",
    "    extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
    "                          'epoch', file_name='loss.png'))\n",
    "trainer.extend(\n",
    "    extensions.PlotReport(\n",
    "        ['main/accuracy', 'validation/main/accuracy'],\n",
    "        'epoch', file_name='accuracy.png'))\n",
    "\n",
    "# Print selected entries of the log to stdout\n",
    "# Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
    "# \"validation\" refers to the default name of the Evaluator extension.\n",
    "# Entries other than 'epoch' are reported by the Classifier link, called by\n",
    "# either the updater or the evaluator.\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'main/loss', 'validation/main/loss',\n",
    "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "\n",
    "# Print a progress bar to stdout\n",
    "trainer.extend(extensions.ProgressBar())\n",
    "\n",
    "# if args.resume:\n",
    "#     # Resume from a snapshot\n",
    "#     chainer.serializers.load_npz(args.resume, trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "1           1762.83     1181.99                                                        5.0672        \n",
      "2           690.236     867.108                                                        9.96964       \n",
      "     total [######............................................] 12.69%\n",
      "this epoch [##########################........................] 53.81%\n",
      "       100 iter, 2 epoch / 20 epochs\n",
      "       inf iters/sec. Estimated time to finish: 0:00:00.\n",
      "3           683.935     855.176                                                        14.938        \n",
      "4           603.466     511.949                                                        19.7251       \n",
      "5           536.636     448.725                                                        24.5303       \n",
      "     total [############......................................] 25.38%\n",
      "this epoch [###...............................................]  7.61%\n",
      "       200 iter, 5 epoch / 20 epochs\n",
      "    8.0625 iters/sec. Estimated time to finish: 0:01:12.930458.\n",
      "6           506.556     491.098                                                        29.4622       \n",
      "7           494.809     449.603                                                        34.3889       \n",
      "     total [###################...............................] 38.07%\n",
      "this epoch [##############################....................] 61.42%\n",
      "       300 iter, 7 epoch / 20 epochs\n",
      "    8.0667 iters/sec. Estimated time to finish: 0:01:00.495657.\n",
      "8           450.814     523.584                                                        39.3771       \n",
      "9           464.765     469.305                                                        44.19         \n",
      "10          454.778     428.773                                                        50.6844       \n",
      "     total [#########################.........................] 50.76%\n",
      "this epoch [#######...........................................] 15.23%\n",
      "       400 iter, 10 epoch / 20 epochs\n",
      "     7.603 iters/sec. Estimated time to finish: 0:00:51.032504.\n",
      "11          429.372     457.432                                                        57.9197       \n",
      "12          429.817     432.991                                                        62.797        \n",
      "     total [###############################...................] 63.45%\n",
      "this epoch [##################################................] 69.04%\n",
      "       500 iter, 12 epoch / 20 epochs\n",
      "    7.4432 iters/sec. Estimated time to finish: 0:00:38.693238.\n",
      "13          391.68      469.199                                                        67.9213       \n",
      "14          404.973     438.336                                                        73.0433       \n",
      "15          402.08      428.372                                                        79.6319       \n",
      "     total [######################################............] 76.14%\n",
      "this epoch [###########.......................................] 22.84%\n",
      "       600 iter, 15 epoch / 20 epochs\n",
      "    7.2264 iters/sec. Estimated time to finish: 0:00:26.015620.\n",
      "16          391.869     434.403                                                        87.872        \n",
      "17          390.145     434.971                                                        93.0546       \n",
      "     total [############################################......] 88.83%\n",
      "this epoch [######################################............] 76.65%\n",
      "       700 iter, 17 epoch / 20 epochs\n",
      "    7.0907 iters/sec. Estimated time to finish: 0:00:12.410666.\n",
      "18          351.656     470.923                                                        98.7398       \n",
      "19          377.935     438.118                                                        105.324       \n",
      "20          376.324     432.801                                                        112.94        \n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref: 188, Pred: 0 id_test: 188\n",
      "Ref: 189, Pred: 0 id_test: 199\n",
      "Ref: 156, Pred: 0 id_test: 371\n",
      "Ref: 167, Pred: 0 id_test: 214\n",
      "Ref: 137, Pred: 0 id_test: 140\n",
      "Ref: 140, Pred: 0 id_test: 39\n",
      "Ref: 151, Pred: 0 id_test: 63\n",
      "Ref: 94, Pred: 0 id_test: 19\n",
      "Ref: 103, Pred: 0 id_test: 296\n",
      "Ref: 82, Pred: 0 id_test: 146\n"
     ]
    }
   ],
   "source": [
    "# test_iter.reset()\n",
    "\n",
    "test_batch = test_iter.next()\n",
    "test_spc, test_ref = concat_examples(test_batch)  # Test Dataset\n",
    "for i in range(10):\n",
    "    ref = test_ref[i]\n",
    "    pred = numpy.argmax(net(test_spc[i].reshape(1, -1)).data)\n",
    "    print(\"Ref: %d, Pred: %d id_test: %s\" % (ref, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([111.], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ref[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[159.69165]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(test_spc[0].reshape(1, -1)).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref: 97, Pred: 100 [[False]]\n",
      "Ref: 222, Pred: 187 [[False]]\n",
      "Ref: 109, Pred: 115 [[False]]\n",
      "Ref: 165, Pred: 184 [[False]]\n",
      "Ref: 205, Pred: 164 [[False]]\n",
      "Ref: 172, Pred: 193 [[False]]\n",
      "Ref: 167, Pred: 148 [[False]]\n",
      "Ref: 142, Pred: 151 [[False]]\n",
      "Ref: 160, Pred: 153 [[False]]\n",
      "Ref: 103, Pred: 102 [[False]]\n"
     ]
    }
   ],
   "source": [
    "test_iter.reset()\n",
    "\n",
    "test_batch = test_iter.next()\n",
    "test_spc, test_ref = concat_examples(test_batch)  # Test Dataset\n",
    "for i in range(10):\n",
    "    ref = test_ref[i]\n",
    "    pred = net(test_spc[i].reshape(1, -1)).data\n",
    "    print(\"Ref: %d, Pred: %d %s\" % (ref, pred, ref == pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_train = numpy.array([97, 222, 109, 165, 205, 172, 167, 142, 160, 103])\n",
    "arr_test = numpy.array([100, 187, 115, 184, 164, 193, 148, 151, 153, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(arr_train, arr_test, 'mo')\n",
    "plt.grid()\n",
    "plt.xlabel('Reference')\n",
    "plt.ylabel('predict')\n",
    "plt.title('blood_glucose_level:validation')\n",
    "plt.text(100,160, 'r = 0.86', fontsize=18, color='r')\n",
    "plt.savefig('val_result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8648281590142726\n"
     ]
    }
   ],
   "source": [
    "cor = numpy.corrcoef(arr_train, arr_test)[0, 1]\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref: 169, Pred: 179 [[False]]\n",
      "Ref: 97, Pred: 88 [[False]]\n",
      "Ref: 103, Pred: 121 [[False]]\n",
      "Ref: 188, Pred: 182 [[False]]\n",
      "Ref: 203, Pred: 179 [[False]]\n",
      "Ref: 160, Pred: 160 [[False]]\n",
      "Ref: 129, Pred: 138 [[False]]\n",
      "Ref: 150, Pred: 167 [[False]]\n",
      "Ref: 119, Pred: 144 [[False]]\n",
      "Ref: 111, Pred: 116 [[False]]\n"
     ]
    }
   ],
   "source": [
    "train_iter.reset()\n",
    "\n",
    "train_batch = train_iter.next()\n",
    "train_spc, train_ref = concat_examples(train_batch)  # Test Dataset\n",
    "for i in range(10):\n",
    "    ref = train_ref[i]\n",
    "    pred = net(train_spc[i].reshape(1, -1)).data\n",
    "    print(\"Ref: %d, Pred: %d %s\" % (ref, pred, ref == pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,) (10,)\n"
     ]
    }
   ],
   "source": [
    "tarr_ref = numpy.array([169, 97, 103, 188, 203, 160, 129, 150, 119, 111])\n",
    "tarr_pre = numpy.array([179, 88, 121, 182, 179, 154, 114, 97, 101, 110])\n",
    "print(tarr_ref.shape, tarr_pre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9347270880905103\n"
     ]
    }
   ],
   "source": [
    "tcor = numpy.corrcoef(tarr_ref, tarr_pre)[0, 1]\n",
    "print(tcor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tarr_ref, tarr_pre, 'ro')\n",
    "plt.grid()\n",
    "plt.xlabel('Reference')\n",
    "plt.ylabel('predict')\n",
    "plt.title('blood_glucose_level:calibration')\n",
    "plt.text(100,160, 'r = 0.93', fontsize=18, color='b')\n",
    "plt.savefig('val_result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラス定義の仕方を変えてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyChain(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(MyChain, self).__init__(\n",
    "            l1 = L.Linear(1000, 1000),\n",
    "            l2 = L.Linear(1000, 1)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x, t):\n",
    "        pt = self.fwd(x)\n",
    "        return F.mean_squared_error\n",
    "    \n",
    "    def fwd(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        return self.l2(h1)\n",
    "    \n",
    "\n",
    "# ダメだった…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 和田さんが昔書いたらしいRegressionクラス\n",
    "#### どれも失敗で終わりそう…誰か助けて…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_beta2(link.Chain):\n",
    "    compute_score = True\n",
    "    \n",
    "    def __init__(self, net, gpu_enable=-1, acc=0, \n",
    "                 lossfun=F.mean_squared_error,\n",
    "                 scorefun=F.r2_score,):\n",
    "        super(Regression_beta2, self).__init__()\n",
    "        self.lossfun = lossfun\n",
    "        self.scorefun = scorefun\n",
    "        self.y = None\n",
    "        self.loss = None\n",
    "        self.score = None\n",
    "        with self.init_scope():\n",
    "            self.net = net\n",
    "            \n",
    "    def __call__(self, x, y):\n",
    "        self.h = None\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        self.h = self.net(x)\n",
    "        self.loss = self.lossfun(self.h, y)\n",
    "        chainer.reporter.report({'loss': self.loss}, self)\n",
    "        if self.compute_score:\n",
    "            self.score = self.scorefun(self.h, y)\n",
    "            chainer.reporter.report({'score': self.score}, self)\n",
    "            return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Sequential(\n",
    "  L.Linear(None, 1000),\n",
    "  F.relu,\n",
    "  L.Linear(1000,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = Regression_beta2(net, \n",
    "                    lossfun=F.mean_squared_error,\n",
    "                     scorefun=F.r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = L.Classifier(net,\n",
    "                       lossfun = F.mean_squared_error,\n",
    "                       accfun=F.r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.random.random((100, 1713)).astype(numpy.float32)   \n",
    "t = numpy.random.random((100,1)).astype(numpy.float32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.sgd.SGD at 0x21676083a20>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = chainer.optimizers.SGD()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = training.updaters.StandardUpdater(x, optimizer)\n",
    "trainer = training.Trainer(updater, (20, 'epoch'), out=\"Result2018_oono/%s\" % time.strftime(\"%Y%m%d%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with the test dataset for each epoch\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))\n",
    "\n",
    "# Dump a computational graph from 'loss' variable at the first iteration\n",
    "# The \"main\" refers to the target link of the \"main\" optimizer.\n",
    "trainer.extend(extensions.dump_graph('main/loss'))\n",
    "\n",
    "# Take a snapshot for each specified epoch\n",
    "# frequency = args.epoch if args.frequency == -1 else max(1, args.frequency)\n",
    "# trainer.extend(extensions.snapshot(), trigger=(frequency, 'epoch'))\n",
    "\n",
    "# Write a log of evaluation statistics for each epoch\n",
    "trainer.extend(extensions.LogReport())\n",
    "\n",
    "# Save two plot images to the result dir\n",
    "trainer.extend(\n",
    "    extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
    "                          'epoch', file_name='loss.png'))\n",
    "trainer.extend(\n",
    "    extensions.PlotReport(\n",
    "        ['main/accuracy', 'validation/main/accuracy'],\n",
    "        'epoch', file_name='accuracy.png'))\n",
    "\n",
    "# Print selected entries of the log to stdout\n",
    "# Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
    "# \"validation\" refers to the default name of the Evaluator extension.\n",
    "# Entries other than 'epoch' are reported by the Classifier link, called by\n",
    "# either the updater or the evaluator.\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'main/loss', 'validation/main/loss',\n",
    "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "\n",
    "# Print a progress bar to stdout\n",
    "trainer.extend(extensions.ProgressBar())\n",
    "\n",
    "# if args.resume:\n",
    "#     # Resume from a snapshot\n",
    "#     chainer.serializers.load_npz(args.resume, trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in main training loop: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\chainer\\training\\trainer.py\", line 312, in run\n",
      "    while not stop_trigger(self):\n",
      "  File \"C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\chainer\\training\\triggers\\interval_trigger.py\", line 51, in __call__\n",
      "    epoch_detail = updater.epoch_detail\n",
      "  File \"C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\chainer\\training\\updaters\\standard_updater.py\", line 101, in epoch_detail\n",
      "    return self._iterators['main'].epoch_detail\n",
      "Will finalize trainer extensions and updater before reraising the exception.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chainer\\training\\trainer.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         'reraising the exception.\\n')\n\u001b[1;32m--> 329\u001b[1;33m             \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    692\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chainer\\training\\trainer.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstop_trigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chainer\\training\\triggers\\interval_trigger.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'epoch'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mepoch_detail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdater\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_detail\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mprevious_epoch_detail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_previous_epoch_detail\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chainer\\training\\updaters\\standard_updater.py\u001b[0m in \u001b[0;36mepoch_detail\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mepoch_detail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'main'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_detail\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-041e2033e90a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chainer\\training\\trainer.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[0;32m    333\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mfinalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdater\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_elapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chainer\\training\\updaters\\standard_updater.py\u001b[0m in \u001b[0;36mfinalize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \"\"\"\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mitervalues\u001b[1;34m(d, **kw)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mitervalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.links.model.classifier.Classifier at 0x21676023c18>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任意のデータセットを取り出したい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132.0\n",
      "151.0\n",
      "153.0\n",
      "147.0\n",
      "138.0\n",
      "136.0\n",
      "95.0\n",
      "121.0\n",
      "120.0\n",
      "95.0\n",
      "178.0\n",
      "175.0\n",
      "113.0\n",
      "104.0\n",
      "131.0\n",
      "178.0\n",
      "111.0\n",
      "137.0\n",
      "167.0\n",
      "146.0\n",
      "111.0\n",
      "113.0\n",
      "189.0\n",
      "131.0\n",
      "127.0\n",
      "146.0\n",
      "169.0\n",
      "107.0\n",
      "120.0\n",
      "122.0\n",
      "121.0\n",
      "117.0\n",
      "141.0\n"
     ]
    }
   ],
   "source": [
    "r = len(y_test)\n",
    "\n",
    "for i in range(r):\n",
    "    print(y_test[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 97.  87. 111.  89.  89. 140. 150. 134. 142. 145. 146. 145. 146. 136.\n",
      " 143. 159. 133. 147. 158. 143. 146. 142. 139. 137. 138. 140. 138. 146.\n",
      " 136. 135. 131. 122. 126. 134. 129.  98.  94.  96.  87.  96.  94.  98.\n",
      " 102.  90.  84.  94.  90. 124. 137. 139. 141. 151. 133. 144. 121. 134.\n",
      " 139. 136. 122. 116. 125. 109. 111. 112. 111. 116. 105. 115. 117. 128.\n",
      " 139. 121. 145. 147. 156. 154. 179. 154. 145. 152. 158. 158. 149. 149.\n",
      " 157. 160. 150. 154. 157. 160. 135. 148. 138. 140. 133. 135. 124. 133.\n",
      " 134. 120. 114. 117.  91.  97.  98.  98. 109. 109. 119. 117. 118. 132.\n",
      " 142. 153. 147. 148. 152. 151. 135. 143. 135. 152. 149. 139. 135. 153.\n",
      " 132. 140. 139. 132. 131. 117. 130. 122. 122. 120. 120.  90.  90.  96.\n",
      "  89.  89.  95.  96.  87.  99.  86.  96.  88.  91. 101. 168. 158. 153.\n",
      " 145. 151. 155. 145. 144. 156. 156. 151. 160. 136. 147. 141. 141. 138.\n",
      " 143. 147. 127. 154. 131. 120. 121. 131. 128. 129. 132. 136. 117. 121.\n",
      " 117. 120.  88.  95.  99.  98. 107. 113. 113. 131. 121. 116. 137. 122.\n",
      " 111. 146. 142. 100. 104. 108. 100. 161. 167. 146. 164. 169. 167. 170.\n",
      " 178. 186. 198. 188. 175. 188. 178. 186. 174. 178. 189. 156. 167. 168.\n",
      " 148. 135. 146. 137. 140. 151. 148. 142. 144.  95.  94. 103.  82.  93.\n",
      " 155. 169. 178. 175. 183. 181. 180. 183. 173. 193. 173. 168. 191. 170.\n",
      " 147. 167. 157. 180. 152. 155. 142. 158. 148. 135. 143. 142. 138. 128.\n",
      " 126. 131. 106. 101. 103.  91.  96.  99. 108.  95.  93.  97.  91. 110.\n",
      " 100.  99.  95. 185. 181. 182. 177. 180. 162. 171. 152. 159. 143. 171.\n",
      " 154. 147. 150. 142. 136. 143. 136. 131. 127. 125. 117. 118. 129. 131.\n",
      " 133. 145. 149. 146. 143. 131. 137. 144. 134. 133. 143. 143. 147. 138.\n",
      " 148. 140. 151. 148. 145. 100. 124. 141. 135. 106. 101. 103.  91.  96.\n",
      "  99. 108.  95.  93.  97.  91. 110. 100.  99.  95. 185. 181. 182. 177.\n",
      " 180. 162. 171. 152. 169. 159. 152. 143. 171. 154. 147. 142. 136. 143.\n",
      " 136. 131. 127. 125. 117. 129. 131. 133. 145. 149. 146. 143. 131. 137.\n",
      " 144. 134. 133. 143. 143. 147. 138. 148. 140. 151. 148. 145. 100. 124.\n",
      " 141. 135. 111. 117. 113. 114. 167. 163. 185. 181. 183. 185. 210. 214.\n",
      " 190. 200. 191. 210. 196. 194. 204. 197. 196. 190. 220. 175. 170. 197.\n",
      " 207. 188. 173. 195. 174. 180. 166.]\n",
      "(427,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Desktop\\LAB\\LAB 2018\\jupyter File\\easy_chainer.py:87: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  data = xls.as_matrix()[:-1]\n",
      "C:\\Users\\Owner\\Desktop\\LAB\\LAB 2018\\jupyter File\\easy_chainer.py:88: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  teach = xls.as_matrix()[-1]\n"
     ]
    }
   ],
   "source": [
    "data, teach = easy_chainer.load_Data(\"C:/Users/Owner/Desktop/Normalized/blood_glucose_fulldata.xlsx\")\n",
    "data = data.astype(numpy.float32)\n",
    "teach = teach\n",
    "print(teach)\n",
    "print(teach.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_train :  (394,)\n",
      "id_train_type :  int32\n",
      "id_test :  (33,)\n",
      "id_test :  int32\n"
     ]
    }
   ],
   "source": [
    "id_train_1 = teach[0:394]\n",
    "id_test_1 = teach[394:427]\n",
    "\n",
    "id_train = id_train_1.astype(numpy.int32)\n",
    "id_test = id_test_1.astype(numpy.int32)\n",
    "\n",
    "# teach = teach.astype(numpy.float32)\n",
    "\n",
    "# print(\"id_train : \", id_train)\n",
    "print(\"id_train : \", id_train.shape)\n",
    "print(\"id_train_type : \", id_train.dtype)\n",
    "\n",
    "# print(\"id_test : \", id_test)\n",
    "print(\"id_test : \", id_test.shape)\n",
    "print(\"id_test : \", id_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teach[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.      , 1.      ],\n",
       "       [0.999609, 0.999732],\n",
       "       [0.998718, 0.998953],\n",
       "       ...,\n",
       "       [0.999004, 1.022624],\n",
       "       [1.000806, 1.024411],\n",
       "       [1.00206 , 1.025625]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = data[:, 0:394], teach[0:394]\n",
    "x_test, y_test = data[:, 394:427], teach[394:427]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tuple_dataset.TupleDataset(x_train.T, y_train.reshape(-1,1 ))\n",
    "test = tuple_dataset.TupleDataset(x_test.T, y_test.reshape(-1, 1))\n",
    "\n",
    "train_iter = chainer.iterators.SerialIterator(train, 10, repeat=True,  shuffle=False)\n",
    "test_iter = chainer.iterators.SerialIterator(test, 10, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP2(1000,1)\n",
    "model = L.Classifier(net,\n",
    "                     lossfun=F.mean_squared_error,\n",
    "                     accfun = F.r2_score)\n",
    "model.compute_accuracy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0x24f0bff5e10>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = training.updaters.StandardUpdater(train_iter, optimizer)\n",
    "trainer = training.Trainer(updater, (5, 'epoch'), out=\"Result2018_oono/%s\" % time.strftime(\"%Y%m%d%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with the test dataset for each epoch\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))\n",
    "\n",
    "# Dump a computational graph from 'loss' variable at the first iteration\n",
    "# The \"main\" refers to the target link of the \"main\" optimizer.\n",
    "trainer.extend(extensions.dump_graph('main/loss'))\n",
    "\n",
    "# Take a snapshot for each specified epoch\n",
    "# frequency = args.epoch if args.frequency == -1 else max(1, args.frequency)\n",
    "# trainer.extend(extensions.snapshot(), trigger=(frequency, 'epoch'))\n",
    "\n",
    "# Write a log of evaluation statistics for each epoch\n",
    "trainer.extend(extensions.LogReport())\n",
    "\n",
    "# Save two plot images to the result dir\n",
    "trainer.extend(\n",
    "    extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
    "                          'epoch', file_name='loss.png'))\n",
    "trainer.extend(\n",
    "    extensions.PlotReport(\n",
    "        ['main/accuracy', 'validation/main/accuracy'],\n",
    "        'epoch', file_name='accuracy.png'))\n",
    "\n",
    "# Print selected entries of the log to stdout\n",
    "# Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
    "# \"validation\" refers to the default name of the Evaluator extension.\n",
    "# Entries other than 'epoch' are reported by the Classifier link, called by\n",
    "# either the updater or the evaluator.\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'main/loss', 'validation/main/loss',\n",
    "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "\n",
    "# Print a progress bar to stdout\n",
    "trainer.extend(extensions.ProgressBar())\n",
    "\n",
    "# if args.resume:\n",
    "#     # Resume from a snapshot\n",
    "#     chainer.serializers.load_npz(args.resume, trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "1           2892.57     1010.58                                                        5.4546        \n",
      "2           2372.61     7001.71                                                        11.4489       \n",
      "     total [#########################.........................] 50.76%\n",
      "this epoch [##########################........................] 53.81%\n",
      "       100 iter, 2 epoch / 5 epochs\n",
      "       inf iters/sec. Estimated time to finish: 0:00:00.\n",
      "3           2540.57     850.967                                                        17.1815       \n",
      "4           1809.13     1151.42                                                        22.6461       \n",
      "5           1969.66     3103.86                                                        27.6999       \n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97, 199 \n",
      "87, 198 \n",
      "111, 193 \n",
      "89, 196 \n",
      "89, 201 \n",
      "140, 201 \n",
      "150, 194 \n",
      "134, 191 \n",
      "142, 197 \n",
      "145, 202 \n"
     ]
    }
   ],
   "source": [
    "# 検証（訓練データ）\n",
    "train_iter.reset()\n",
    "\n",
    "train_batch = train_iter.next()\n",
    "train_spc, train_ref = concat_examples(train_batch)  # Test Dataset\n",
    "for i in range(10):\n",
    "    cal_ref = train_ref[i]\n",
    "    cal_pred = net(train_spc[i].reshape(1, -1)).data\n",
    "    print(\"%d, %d \" % (cal_ref, cal_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174, 222 \n",
      "180, 246 \n",
      "166, 221 \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-a78b6947ddc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_spc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat_examples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Test Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_ref\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_spc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%d, %d \"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "# 検証（テストデータ）\n",
    "#test_iter.reset()\n",
    "\n",
    "test_batch = test_iter.next()\n",
    "test_spc, test_ref = concat_examples(test_batch)  # Test Dataset\n",
    "for i in range(10):\n",
    "    ref = test_ref[i]\n",
    "    pred = net(test_spc[i].reshape(1, -1)).data\n",
    "    print(\"%d, %d \" % (ref, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
